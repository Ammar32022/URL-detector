# app_streamlit.py
import streamlit as st
import pandas as pd
import joblib
from urllib.parse import urlparse
import ipaddress
import io

st.set_page_config(page_title="ÙƒØ´Ù Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¶Ø§Ø±Ø©", layout="centered")

st.title("ğŸš¨ ÙƒØ§Ø´Ù Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¶Ø§Ø±Ø© (AI)")

########################
# Ø¯ÙˆØ§Ù„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø®ØµØ§Ø¦Øµ
########################
def extract_features_single_advanced(url: str):
    url = str(url).strip()
    features = {}
    features["url_length"] = len(url)
    features["count_dots"] = url.count('.')
    features["count_hyphens"] = url.count('-')
    features["count_digits"] = sum(c.isdigit() for c in url)
    features["has_https"] = 1 if url.lower().startswith("https://") else 0
    try:
        features["domain_length"] = len(urlparse(url).netloc)
    except:
        features["domain_length"] = 0
    features["count_slashes"] = url.count('/')
    try:
        netloc = urlparse(url).netloc
        # netloc may include port -> remove port
        netloc_no_port = netloc.split(':')[0]
        ipaddress.ip_address(netloc_no_port)
        features["has_ip"] = 1
    except:
        features["has_ip"] = 0
    suspicious_words = ["login","secure","bank","update","verify","confirm","account","password",
                        "paypal","reset","signin","admin","auth"]
    features["suspicious_words"] = sum(word in url.lower() for word in suspicious_words)
    return pd.DataFrame([features])


def extract_features_batch(df_urls: pd.Series):
    rows = []
    for u in df_urls.astype(str):
        rows.append(extract_features_single_advanced(u).iloc[0].to_dict())
    feat_df = pd.DataFrame(rows)
    return feat_df

########################
# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (Ù…Ù† Ø§Ù„Ù…Ù„Ù Ø£Ùˆ Ø±ÙØ¹)
########################
@st.cache_resource
def load_model_from_path(path="xgb_model_advanced.pkl"):
    try:
        model = joblib.load(path)
        return model, f"Loaded model from {path}"
    except Exception as e:
        return None, f"Cannot load model from {path}: {e}"

st.sidebar.header("Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬")
model_path_input = st.sidebar.text_input("Ù…Ø³Ø§Ø± Ù…Ù„Ù Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (Ø§ÙØªØ±Ø§Ø¶ÙŠ)", "xgb_model_advanced.pkl")

model, msg = load_model_from_path(model_path_input)
st.sidebar.write(msg)

# uploader ÙƒØ®ÙŠØ§Ø± Ø¨Ø¯ÙŠÙ„ Ù„Ùˆ Ø§Ù„Ù…Ù„Ù Ù…Ø´ Ù…ÙˆØ¬ÙˆØ¯
uploaded_model = st.sidebar.file_uploader("Ø£Ùˆ Ø§Ø±ÙØ¹ Ù…Ù„Ù Ù†Ù…ÙˆØ°Ø¬ (.pkl)", type=["pkl","joblib"])
if uploaded_model is not None:
    try:
        # joblib.load ÙŠØ¹Ù…Ù„ Ù…Ø¹ file-like ÙÙŠ Ù…Ø¹Ø¸Ù… Ø§Ù„Ø­Ø§Ù„Ø§ØªØ› Ù„Ùˆ ÙØ´Ù„ Ù†Ø¬Ø±Ø¨ pickle
        uploaded_model.seek(0)
        model = joblib.load(uploaded_model)
        st.sidebar.success("ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø±ÙÙˆØ¹")
    except Exception as e:
        st.sidebar.error(f"ÙØ´Ù„ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø±ÙÙˆØ¹: {e}")
        model = None

if model is None:
    st.warning("Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø­Ù…Ù‘Ù„. Ø¥Ù…Ø§ Ø¶Ø¹ Ù…Ù„Ù xgb_model_advanced.pkl ÙÙŠ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø£Ùˆ Ø§Ø±ÙØ¹Ù‡ Ù…Ù† Ø§Ù„Ø´Ø±ÙŠØ· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠ.")
else:
    st.success("Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¬Ø§Ù‡Ø² Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… âœ…")

########################
# ÙˆØ§Ø¬Ù‡Ø© Ø§Ø®ØªØ¨Ø§Ø± Ø±Ø§Ø¨Ø· ÙˆØ§Ø­Ø¯
########################
st.header("Ø§Ø®ØªØ¨Ø§Ø± Ø±Ø§Ø¨Ø· ÙˆØ§Ø­Ø¯")
single_url = st.text_input("Ø£Ø¯Ø®Ù„ Ø§Ù„Ø±Ø§Ø¨Ø· Ù„Ø§Ø®ØªØ¨Ø§Ø±Ù‡", placeholder="https://example.com/..")

col1, col2 = st.columns([1,1])
with col1:
    if st.button("ØªØ­Ù‚Ù‚ Ø§Ù„Ø¢Ù†"):
        if not model:
            st.error("Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù†Ù…ÙˆØ°Ø¬ Ø¬Ø§Ù‡Ø². Ø­Ù…Ù‘Ù„Ù‡ Ø£ÙˆÙ„Ø§Ù‹.")
        elif not single_url:
            st.warning("Ø£Ø¯Ø®Ù„ Ø±Ø§Ø¨Ø·Ù‹Ø§ Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±.")
        else:
            feats = extract_features_single_advanced(single_url).to_numpy()
            try:
                pred = model.predict(feats)[0]
                label = "âš ï¸ Ø¶Ø§Ø±" if int(pred)==1 else "âœ… ØºÙŠØ± Ø¶Ø§Ø±"
                st.info(f"Ø§Ù„Ù†ØªÙŠØ¬Ø©: {label}")
            except Exception as e:
                st.error(f"Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªÙ†Ø¨Ø¤: {e}")

with col2:
    if st.button("ØªÙØµÙŠÙ„ Ø§Ù„Ø®ØµØ§Ø¦Øµ"):
        if not single_url:
            st.warning("Ø£Ø¯Ø®Ù„ Ø±Ø§Ø¨Ø·Ù‹Ø§ Ø£ÙˆÙ„Ø§Ù‹.")
        else:
            st.write(extract_features_single_advanced(single_url).T)

########################
# ÙˆØ§Ø¬Ù‡Ø© ÙØ­Øµ Ù…Ù„Ù CSV (Ø¯ÙØ¹ÙŠ)
########################
st.header("ÙØ­Øµ Ù…Ù„Ù CSV (Ù‚Ø§Ø¦Ù…Ø© Ø±ÙˆØ§Ø¨Ø·)")
st.write("Ø±ÙØ¹ CSV ÙŠØ­ØªÙˆÙŠ Ø¹Ù…ÙˆØ¯ ÙˆØ§Ø­Ø¯ Ø§Ø³Ù…Ù‡ url Ø£Ùˆ Ø¹Ù…ÙˆØ¯ ÙŠØ­ØªÙˆÙŠ Ø±ÙˆØ§Ø¨Ø·. Ø³ØªÙØ¹Ø§Ø¯ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ù…Ø¹ Ø¹Ù…ÙˆØ¯ prediction (0=Ø¢Ù…Ù†,1=Ø¶Ø§Ø±).")
uploaded_csv = st.file_uploader("Ø§Ø±ÙØ¹ Ù…Ù„Ù CSV Ù„Ù„Ø¨Ø­Ø« (ÙŠØ¬Ø¨ Ø£Ù† ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¹Ù…ÙˆØ¯ url Ø£Ùˆ link)", type=["csv"])
if uploaded_csv is not None:
    try:
        df_in = pd.read_csv(uploaded_csv)
        # Ø­Ø§ÙˆÙ„ Ø¥ÙŠØ¬Ø§Ø¯ Ø¹Ù…ÙˆØ¯ URL
        url_col = None
        for c in df_in.columns:
            if c.lower() in ("url","link","uri","website"):
                url_col = c
                break
        if url_col is None:
            # Ø®Ù…Ù† Ø§Ù„Ø¹Ù…ÙˆØ¯ Ø§Ù„Ù„ÙŠ ÙŠØ­ØªÙˆÙŠ Ø±ÙˆØ§Ø¨Ø· Ø¨ÙØ­Øµ Ø§Ù„Ø¹ÙŠÙ†Ø§Øª
            candidate = None
            for c in df_in.columns:
                sample = df_in[c].astype(str).head(50).str.lower()
                if sample.str.contains("http://|https://|www\\.|\\.com|\\.net").any():
                    candidate = c
                    break
            url_col = candidate

        if url_col is None:
            st.error("Ù„Ù… Ø£Ø¬Ø¯ Ø¹Ù…ÙˆØ¯ Ø±ÙˆØ§Ø¨Ø· ÙÙŠ CSV. ØªØ£ÙƒØ¯ Ø£Ù† Ø§Ù„Ù…Ù„Ù ÙŠØ­ØªÙˆÙŠ Ø¹Ù…ÙˆØ¯Ù‹Ø§ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø±ÙˆØ§Ø¨Ø·.")
        else:
            st.write(f"Ø§Ø³ØªØ®Ø¯Ù…Øª Ø§Ù„Ø¹Ù…ÙˆØ¯: {url_col}")
            urls_series = df_in[url_col].astype(str)
            feat_df = extract_features_batch(urls_series)
            if model is None:
                st.error("Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")
            else:
                preds = model.predict(feat_df.to_numpy())
                df_out = df_in.copy()
                df_out["prediction"] = preds.astype(int)
                st.success("ØªÙ… Ø§Ù„ØªÙ†Ø¨Ø¤! ÙŠÙ…ÙƒÙ†Ùƒ Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø£Ø¯Ù†Ø§Ù‡.")
                st.dataframe(df_out.head(200))
                # ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
                towrite = io.BytesIO()
                df_out.to_csv(towrite, index=False)
                towrite.seek(0)
                st.download_button("â¬‡ï¸ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙƒÙ€ CSV", towrite, file_name="urls_predictions.csv", mime="text/csv")
    except Exception as e:
        st.error(f"Ø®Ø·Ø£ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù: {e}")

########################
# Ø´Ø±ÙˆØ­Ø§Øª ÙˆÙ…Ù„Ø§Ø­Ø¸Ø§Øª
########################
st.markdown("---")
st.markdown("### Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ù…Ù‡Ù…Ø©")
st.markdown("""
- Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ø§ ÙŠØ²ÙˆØ± Ø§Ù„Ø±ÙˆØ§Ø¨Ø· ÙˆÙ„Ø§ ÙŠÙ†ÙÙ‘Ø°Ù‡Ø§ â€” ÙÙ‚Ø· ÙŠØ­Ù„Ù„ Ù†Øµ Ø§Ù„Ø±Ø§Ø¨Ø· (Ù…ÙÙŠØ¯ Ù„Ù„Ø­Ù…Ø§ÙŠØ© ÙˆØ§Ù„Ø®ØµÙˆØµÙŠØ©).  
- Ù„Ùˆ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø®Ø§Ø·Ø¦Ø© Ù„Ø¨Ø¹Ø¶ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·ØŒ Ø¬Ø±Ù‘Ø¨ ØªØ­Ø³ÙŠÙ† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ùˆ Ø¥Ø¶Ø§ÙØ© Ø®ØµØ§Ø¦Øµ Ø¬Ø¯ÙŠØ¯Ø©.  
- Ù„ØªØ´ØºÙŠÙ„ Ù‡Ø°Ø§ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ù†ØªØ±Ù†ØªØŒ Ø§Ù†Ø´Ø± Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø¹Ù„Ù‰ GitHub ÙˆØ§Ø±Ø¨Ø·Ù‡ Ø¨Ù€ Streamlit Cloud (Ø´Ø±Ø­ Ø¨Ø§Ù„Ø£Ø³ÙÙ„).  
""")